{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose of this notebook:\n",
    "The purpose of this notebook is to create a brainplot for the recall interval\n",
    "\n",
    "Made March 6 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not import MorletWaveletFilterCppLegacy (single-core C++ version of MorletWaveletFilter): cannot import name MorletWaveletTransform\n",
      "You can still use MorletWaveletFilter\n"
     ]
    }
   ],
   "source": [
    "#Plotting!\n",
    "%matplotlib notebook\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# General Imports\n",
    "import os, sys\n",
    "from glob import glob # Glob.glob is more fun to say\n",
    "from time import time \n",
    "import cluster_helper.cluster  # Parallelize by the same old antics\n",
    "\n",
    "# Add some dimension to your life, np=arr, pd=labeled arr, xr=labeled N-D arr\n",
    "import numpy as np # I say numpy like 'lumpy', no I don't mean num-pie\n",
    "import pandas as pd # For Undergrads that say \"But matlab makes it so much easier to view things\"\n",
    "import xarray as xr # N-D labels!\n",
    "\n",
    "# Pizza ptsa stuff\n",
    "from ptsa.data.TimeSeriesX import TimeSeriesX\n",
    "from ptsa.data.readers import BaseEventReader, TalReader, EEGReader, JsonIndexReader\n",
    "\n",
    "# EEG Toolbox NEW WAY OF IMPORTING!!!!\n",
    "sys.path.append('/home2/loganf/SecondYear/Functions/CML/')\n",
    "from GetData import get_sub_events, get_intrusions, get_sub_tal, get_sub_eeg, get_subs\n",
    "from Utility.FrequencyCreator import logspace \n",
    "from SpectralAnalysis.RollingAverage import sliding_mean_fast\n",
    "#from Utility.InclusionHelperFunctions import set_new_fields, apply_inclusion_criteria, get_valid_trials \n",
    "#from Utility.InclusionHelperFunctions import get_roi_indexes, morlet, \n",
    "from Utility.InclusionHelperFunctions import append_field_workaround\n",
    "from Utility.RetrievalDeliberationCreation import get_matched_retrievals\n",
    "from Stats.ZScoreFromWholeSession import BurkeZscoreNormalization\n",
    "#from SpectralAnalysis.MentalChronometry import MentalChronometry\n",
    "#from Utility.ZScoreMethods import ZScoreMethods\n",
    "\n",
    "\n",
    "#Plotting!\n",
    "%matplotlib notebook\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# General Imports\n",
    "import os, sys\n",
    "\n",
    "from glob import glob # Glob.glob is more fun to say\n",
    "from time import time \n",
    "import cluster_helper.cluster  # Parallelize by the same old antics\n",
    "\n",
    "# EEG Toolbox NEW WAY OF IMPORTING!!!!\n",
    "sys.path.append('/home2/loganf/SecondYear/Functions/CML/')\n",
    "from GetData import get_sub_events, get_intrusions, get_sub_tal, get_sub_eeg, get_subs\n",
    "from BrainPlots.BrainActivityWriter import Label\n",
    "from BrainPlots.SphereBrainParc import get_avg_surface_spheres, get_avg_surface_spheres_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Load_Data(savedfilename):\n",
    "    \"\"\"\n",
    "    The purpose of this code is to be able to quickly load any kind of data\n",
    "    using pickle.\n",
    "    \"\"\"\n",
    "    import pickle\n",
    "\n",
    "    try:\n",
    "\n",
    "        with open(savedfilename, 'rb') as handle:\n",
    "            loaded_data = pickle.load(handle)\n",
    "        \n",
    "        return loaded_data\n",
    "    \n",
    "    except:\n",
    "        import numpy as np\n",
    "        loaded_data = np.load(savedfilename)\n",
    "        return loaded_data\n",
    "    \n",
    "def load_spheres_correct_index(subject, bp):\n",
    "    \n",
    "    #channel_paths = '/scratch/loganf/electrode_information/{}/{}.npy'.format(subject, 'bp')   \n",
    "    #if os.path.exists(channel_paths): \n",
    "        #bp = np.load(channel_paths)\n",
    "        \n",
    "    bp_to_index = {i:bp[i] for i in xrange(len(bp))}\n",
    "    sphere_path = '/scratch/loganf/spheres/{}_FR1'.format(subject)\n",
    "    sphere_path = Load_Data(sphere_path)\n",
    "    \n",
    "    for k,v in enumerate(sphere_path):\n",
    "        sphere_path[v] = np.array([bp_to_index[j] for i,j in enumerate(sphere_path[v]) if j in bp_to_index.keys()])\n",
    "    return sphere_path\n",
    "\n",
    "def update_spheres(sphere_d, data):\n",
    "    new_d = {}\n",
    "    for k,v in enumerate(sphere_d):\n",
    "        in_sphere = data.sel(bipolar_pairs=sphere_d[v])\n",
    "        if len(in_sphere.data) > 0:\n",
    "            new_d[v] = in_sphere.data\n",
    "        #else:\n",
    "            #sphere_d.pop(v)\n",
    "        #if in_sphere.data.shape == (200,):\n",
    "            #sphere_d[v] = in_sphere.data\n",
    "        #else:\n",
    "            #sphere_d[v] = np.nanmean(in_sphere.data,0)\n",
    "    return new_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import path as op\n",
    "import time\n",
    "import numpy as np\n",
    "from nibabel.freesurfer.io import read_geometry\n",
    "from six import b\n",
    "\n",
    "from scipy import sparse\n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "\n",
    "import nibabel as nib\n",
    "import os\n",
    "\n",
    "\n",
    "class Label(object):\n",
    "    \"\"\"Code Written by Jim Kregal for writing annotation labels to the surface of an averaged suubject brain\"\"\"\n",
    "\n",
    "    def __init__(self, tal, pos=None, values=None, hemi=None, comment=\"\",\n",
    "                 name=None, outdir=None, subject=None, verbose=None,\n",
    "                 basedir='/data/eeg/freesurfer/subjects/', radius=3.0,\n",
    "                 stat=None, smooth=0.0, x=None, y=None, z=None):\n",
    "        if hemi == 'lh':\n",
    "            self.surf = basedir + 'average/surf/lh.pial'\n",
    "\n",
    "        elif hemi == 'rh':\n",
    "            self.surf = basedir + 'average/surf/rh.pial'\n",
    "\n",
    "        if x is None:\n",
    "            x = tal['atlases']['avg.dural']['x']\n",
    "\n",
    "        if y is None:\n",
    "            y = tal['atlases']['avg.dural']['y']\n",
    "\n",
    "        if z is None:\n",
    "            z = tal['atlases']['avg.dural']['z']\n",
    "\n",
    "        pos = np.vstack((x, y, z)).T\n",
    "\n",
    "        # need to filter all the electrodes by hemisphere\n",
    "        if hemi == 'lh':\n",
    "            stat = stat[pos[:, 0] < 0]\n",
    "            pos = pos[pos[:, 0] < 0]\n",
    "        elif hemi == 'rh':\n",
    "            stat = stat[pos[:, 0] > 0]\n",
    "            pos = pos[pos[:, 0] > 0]\n",
    "\n",
    "        # TODO: may want to exclude depth electrodes\n",
    "        # TODO: may want to have minimum distance to surface\n",
    "\n",
    "        self.surface = nib.freesurfer.read_geometry(os.path.join(basedir, 'average' + '/surf/' + hemi + '.pial'))\n",
    "        self.cortex = np.sort(\n",
    "            nib.freesurfer.read_label(os.path.join(basedir, 'average' + '/label/' + hemi + '.cortex.label')))\n",
    "        self.sulc = nib.freesurfer.read_morph_data(os.path.join(basedir, 'average' + '/surf/lh.sulc'))\n",
    "\n",
    "        self.pos = pos\n",
    "        self.stat = stat\n",
    "\n",
    "        self.radius = radius\n",
    "        self.hemi = hemi\n",
    "        self.comment = comment\n",
    "        self.verbose = verbose\n",
    "        self.subject = subject\n",
    "        self.name = name\n",
    "        self.outdir = outdir\n",
    "        self.smooth = smooth / 2.35482\n",
    "\n",
    "    def map_stat_to_label(self):\n",
    "\n",
    "        self.coords, faces, vertices = self.get_vertices_from_surf()\n",
    "\n",
    "        fnames = []\n",
    "        elec_vertices = np.empty(self.pos.shape[0], dtype=int)\n",
    "\n",
    "        filename = op.join(self.outdir, self.subject)\n",
    "        path_head, name = op.split(filename)\n",
    "        if name.endswith('.label'):\n",
    "            name = name[:-6]\n",
    "        if not (name.startswith(self.hemi) or name.endswith(self.hemi)):\n",
    "            name += '-' + self.hemi\n",
    "        filename = op.join(path_head, name) + '.label'\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        self.counts = np.full(len(vertices), 0)\n",
    "        self.vertices = vertices\n",
    "        self.values = np.full(len(vertices), 0.)\n",
    "\n",
    "        connectivity = mesh_dist(self.surface[1], self.surface[0])\n",
    "\n",
    "        for i, elec_coord in enumerate(self.pos):\n",
    "            elec_vertices[i] = self.get_nearest_vertex(elec_coord)[0]\n",
    "\n",
    "        elec_dists = sparse.csgraph.dijkstra(connectivity, indices=elec_vertices)\n",
    "\n",
    "        for i, e_dist in enumerate(elec_dists):\n",
    "            self.values[np.where(((e_dist < self.radius) & (e_dist > 0.0)) | (vertices == elec_vertices[i]))] += \\\n",
    "            self.stat[i]\n",
    "            self.counts[np.where(((e_dist < self.radius) & (e_dist > 0.0)) | (vertices == elec_vertices[i]))] += 1\n",
    "\n",
    "        self.values = self.values / self.counts\n",
    "        # TODO: may want to remove vertices, pos, values where there is no data to make a smaller label files\n",
    "        label = write_label(filename, self)\n",
    "        fnames.append(filename)\n",
    "\n",
    "        return fnames\n",
    "\n",
    "    def get_vertices_from_surf(self):\n",
    "        '''\n",
    "\n",
    "        :param hemi:\n",
    "        :return:\n",
    "        '''\n",
    "\n",
    "        coords, faces = read_geometry(self.surf)\n",
    "        vertices = np.arange(coords.shape[0])\n",
    "        return coords, faces, vertices\n",
    "\n",
    "    def get_nearest_vertex(self, elec_coord):\n",
    "        '''\n",
    "\n",
    "        :return:\n",
    "        '''\n",
    "\n",
    "        dist = np.linalg.norm(self.coords - elec_coord, axis=1)\n",
    "        idx = np.where(dist == dist.min())[0]\n",
    "\n",
    "        return idx\n",
    "\n",
    "\n",
    "def write_label(filename, label, verbose=None):\n",
    "    \"\"\"Write a FreeSurfer label.\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : string\n",
    "        Path to label file to produce.\n",
    "    label : Label\n",
    "        The label object to save.\n",
    "    verbose : bool, str, int, or None\n",
    "        If not None, override default verbose level (see :func:`mne.verbose`\n",
    "        and :ref:`Logging documentation <tut_logging>` for more).\n",
    "    Notes\n",
    "    -----\n",
    "    Note that due to file specification limitations, the Label's subject and\n",
    "    color attributes are not saved to disk.\n",
    "    See Also\n",
    "    --------\n",
    "    write_labels_to_annot\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filename, 'wb') as fid:\n",
    "        n_vertices = len(label.vertices)\n",
    "        data = np.zeros((n_vertices, 5), dtype=np.float)\n",
    "        data[:, 0] = label.vertices\n",
    "        data[:, 1:4] = label.coords  # self.pos #1e3 *\n",
    "        data[:, 4] = label.values\n",
    "        fid.write(b(\"#%s\\n\" % label.comment))\n",
    "        fid.write(b(\"%d\\n\" % n_vertices))\n",
    "        for d in data:\n",
    "            fid.write(b(\"%d %f %f %f %f\\n\" % tuple(d)))\n",
    "    return label\n",
    "\n",
    "\n",
    "def average_duplicates(vertices, pos, values):\n",
    "    result_vertices = np.unique(vertices)\n",
    "    result_values = np.empty(result_vertices.shape)\n",
    "    result_pos = np.empty((1, len(result_vertices), 3))\n",
    "\n",
    "    for i, vertex in enumerate(result_vertices):\n",
    "        result_values[i] = np.mean(values[vertices == vertex])\n",
    "        result_pos[0, i, :] = np.mean(pos[0, vertices == vertex, :], axis=0)\n",
    "\n",
    "    return result_vertices, result_pos, result_values\n",
    "\n",
    "\n",
    "def gaussian(dist, sig):\n",
    "    return np.exp(-np.power(dist, 2.) / (2 * np.power(sig, 2.)))\n",
    "\n",
    "\n",
    "def mesh_edges(tris):\n",
    "    \"\"\"Return sparse matrix with edges as an adjacency matrix.\n",
    "    Parameters\n",
    "    ----------\n",
    "    tris : array of shape [n_triangles x 3]\n",
    "        The triangles.\n",
    "    Returns\n",
    "    -------\n",
    "    edges : sparse matrix\n",
    "        The adjacency matrix.\n",
    "    \"\"\"\n",
    "    if np.max(tris) > len(np.unique(tris)):\n",
    "        raise ValueError('Cannot compute connectivity on a selection of '\n",
    "                         'triangles.')\n",
    "\n",
    "    npoints = np.max(tris) + 1\n",
    "    ones_ntris = np.ones(3 * len(tris))\n",
    "\n",
    "    a, b, c = tris.T\n",
    "    x = np.concatenate((a, b, c))\n",
    "    y = np.concatenate((b, c, a))\n",
    "    edges = coo_matrix((ones_ntris, (x, y)), shape=(npoints, npoints))\n",
    "    edges = edges.tocsr()\n",
    "    edges = edges + edges.T\n",
    "    return edges\n",
    "\n",
    "\n",
    "def mesh_dist(tris, vert):\n",
    "    \"\"\"Compute adjacency matrix weighted by distances.\n",
    "    It generates an adjacency matrix where the entries are the distances\n",
    "    between neighboring vertices.\n",
    "    Parameters\n",
    "    ----------\n",
    "    tris : array (n_tris x 3)\n",
    "        Mesh triangulation\n",
    "    vert : array (n_vert x 3)\n",
    "        Vertex locations\n",
    "    Returns\n",
    "    -------\n",
    "    dist_matrix : scipy.sparse.csr_matrix\n",
    "        Sparse matrix with distances between adjacent vertices\n",
    "    \"\"\"\n",
    "    edges = mesh_edges(tris).tocoo()\n",
    "\n",
    "    # Euclidean distances between neighboring vertices\n",
    "    dist = np.linalg.norm(vert[edges.row, :] - vert[edges.col, :], axis=1)\n",
    "    dist_matrix = csr_matrix((dist, (edges.row, edges.col)), shape=edges.shape)\n",
    "    return dist_matrix\n",
    "\n",
    "\n",
    "def Load_Data(savedfilename):\n",
    "    \"\"\"\n",
    "    The purpose of this code is to be able to quickly load any kind of data\n",
    "    using pickle.\n",
    "    \"\"\"\n",
    "    import pickle\n",
    "\n",
    "    try:\n",
    "\n",
    "        with open(savedfilename, 'rb') as handle:\n",
    "            loaded_data = pickle.load(handle)\n",
    "            print\n",
    "            'loaded successfully, fileloaded as as:\\nloaded_data'\n",
    "        return loaded_data\n",
    "    except:\n",
    "        import numpy as np\n",
    "        loaded_data = np.load(savedfilename)\n",
    "        return loaded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paths = glob('/scratch/loganf/spheres_single_time_point/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_range = np.arange(-1, 1, .01)\n",
    "start_stops = zip(test_range-.25, test_range+.5-.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-1.25, -0.75),\n",
       " (-0.74999999999999956, -0.24999999999999956),\n",
       " (-0.24999999999999911, 0.25000000000000089),\n",
       " (0.25000000000000133, 0.75000000000000133)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_stops[::50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_spheres_by_epoch(subject_spheres, epoch=0):\n",
    "    new_d = {}\n",
    "    for k,v in enumerate(subject_spheres):\n",
    "        new_d[v] = np.mean(subject_spheres[v][:,epoch])\n",
    "    return new_d\n",
    "\n",
    "def update_dictionary(growing_dict, new_dict):\n",
    "    for k,v in enumerate(new_dict):\n",
    "        if v not in growing_dict:\n",
    "            growing_dict[v] = []\n",
    "        if v in growing_dict:\n",
    "            growing_dict[v].append(new_dict[v])\n",
    "    return growing_dict\n",
    "\n",
    "from collections import OrderedDict\n",
    "my_d = OrderedDict()\n",
    "\n",
    "for p in paths:\n",
    "    try:\n",
    "        subject_spheres = Load_Data(p)\n",
    "        subject_spheres = filter_spheres_by_epoch(subject_spheres, 0)\n",
    "        update_dictionary(growing_dict = my_d, new_dict=subject_spheres)\n",
    "    except:\n",
    "        print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = OrderedDict()\n",
    "for k,v in enumerate(my_d):\n",
    "    if len(my_d[v]) > 5:\n",
    "        test[v] = np.array(my_d[v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.74794357, -0.1286965 , -0.81919502, -1.95961747,  0.6521079 ,\n",
       "        0.18372324, -0.16267003, -2.29375731,  3.09857347, -2.05628947,\n",
       "       -0.11216792,  0.157666  , -2.77440184,  0.05171172, -3.79001723,\n",
       "        0.12442267])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[test.keys()[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_1samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_values = OrderedDict()\n",
    "\n",
    "\n",
    "for k,v in enumerate(test):\n",
    "    t, p = ttest_1samp(test[v], 0)\n",
    "    p_values[v] = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_test = np.array(p_values.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2329"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.where(p_test < .05)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43641"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append('/home2/loganf/statsmodels/')\n",
    "from statsmodels.sandbox.stats.multicomp import fdrcorrection0 as fdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boolean, corr = fdr(p_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64),)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(corr<.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptsa_fresh",
   "language": "python",
   "name": "ptsa_fresh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
